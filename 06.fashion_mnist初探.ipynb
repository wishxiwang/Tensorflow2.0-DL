{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理方式\n",
    "def prepare_data(x,y):\n",
    "    x = tf.cast(x,dtype=tf.float32)/255.\n",
    "    y = tf.one_hot(y,depth=10)\n",
    "    y=tf.cast(y,dtype=tf.int64)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 28, 28) (128, 10)\n"
     ]
    }
   ],
   "source": [
    "#载入数据和预处理\n",
    "(x_train,y_train),(x_test,y_test)=keras.datasets.fashion_mnist.load_data()\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(128).map(prepare_data).shuffle(10000)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(128).map(prepare_data).shuffle(10000)\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print(sample[0].shape,sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化系数\n",
    "w1 = tf.Variable(tf.random.truncated_normal((784,256),stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros(256))\n",
    "w2 = tf.Variable(tf.random.truncated_normal((256,128),stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal((128,10),stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros(10))\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-83baa448f160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#原地更新,数据类型保持不变,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mb1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mw2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for step,(x,y) in enumerate(train_db):\n",
    "        x = tf.reshape(x,[-1,28*28])\n",
    "        with tf.GradientTape() as tape:\n",
    "            out1 = tf.nn.relu(x@w1 + b1)\n",
    "            out2 = tf.nn.relu(out1@w2+b2)\n",
    "            out3 = tf.cast(out2@w3+b3,dtype=tf.int64)\n",
    "            loss=tf.reduce_mean(tf.square(y-out3))\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        print(grads)\n",
    "        w1.assign_sub(lr * grads[0])    #原地更新,数据类型保持不变,\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss))\n",
    "        total_correct, total_number = 0, 0\n",
    "    for step, (x,y) in enumerate(test_db):\n",
    "\n",
    "        # x_test [b, 28,28] => [b, 28*28]\n",
    "        x_test = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "        # [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "        h1 = tf.nn.relu(x_test@w1 + b1)\n",
    "        h2 = tf.nn.relu(h1@w2 + b2)\n",
    "        out = h2@w3 + b3\n",
    "\n",
    "        # out: [b, 10] 这里的out属于实数的范围R。\n",
    "        # prob: [b, 10]实数范围映射到[0, 1]范围内。\n",
    "        prob = tf.nn.softmax(out, axis=1)  #是在[b, 10]中的10维度上面。故axis=1\n",
    "        # 预测值：选择概率最大的值所在的位置。[b, 10] ==> [b]\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # 真实值：y: [b] 这里我们可以发现做测试的时候，编码方式不需要转换为one-hot,只需要把索引和pred比较。\n",
    "        # [b] int32类型。\n",
    "        # print(pred.dtype, y.dtype) 运行结果：<dtype: 'int64'> <dtype: 'int32'> 类型不匹配\n",
    "        correct = tf.cast(tf.equal(pred, y),dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "\n",
    "        total_correct +=int(correct)        #总的正确的个数，\n",
    "        total_number +=x.shape[0]           #总的测试的个数。\n",
    "\n",
    "\n",
    "    #循环结束以后：\n",
    "    acc = total_correct /total_number\n",
    "    print(\"test acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
      "0 0 loss: 0.35197287797927856\n",
      "0 100 loss: 0.18710939586162567\n",
      "0 200 loss: 0.18492546677589417\n",
      "0 300 loss: 0.15776900947093964\n",
      "0 400 loss: 0.13826946914196014\n",
      "test acc:  0.1716\n",
      "1 0 loss: 0.1342138946056366\n",
      "1 100 loss: 0.14012067019939423\n",
      "1 200 loss: 0.151187464594841\n",
      "1 300 loss: 0.1359231024980545\n",
      "1 400 loss: 0.12222625315189362\n",
      "test acc:  0.2272\n",
      "2 0 loss: 0.11679490655660629\n",
      "2 100 loss: 0.12546411156654358\n",
      "2 200 loss: 0.13325053453445435\n",
      "2 300 loss: 0.12204279005527496\n",
      "2 400 loss: 0.1116311177611351\n",
      "test acc:  0.2759\n",
      "3 0 loss: 0.10520273447036743\n",
      "3 100 loss: 0.11521227657794952\n",
      "3 200 loss: 0.12084205448627472\n",
      "3 300 loss: 0.11205847561359406\n",
      "3 400 loss: 0.10401581227779388\n",
      "test acc:  0.3207\n",
      "4 0 loss: 0.09673832356929779\n",
      "4 100 loss: 0.10757486522197723\n",
      "4 200 loss: 0.11159995943307877\n",
      "4 300 loss: 0.10447409003973007\n",
      "4 400 loss: 0.09816659241914749\n",
      "test acc:  0.3634\n",
      "5 0 loss: 0.0902741551399231\n",
      "5 100 loss: 0.10165836662054062\n",
      "5 200 loss: 0.10443989187479019\n",
      "5 300 loss: 0.09846678376197815\n",
      "5 400 loss: 0.09352438896894455\n",
      "test acc:  0.4091\n",
      "6 0 loss: 0.08512043207883835\n",
      "6 100 loss: 0.09689817577600479\n",
      "6 200 loss: 0.09865014255046844\n",
      "6 300 loss: 0.09361210465431213\n",
      "6 400 loss: 0.08968700468540192\n",
      "test acc:  0.4434\n",
      "7 0 loss: 0.08092360198497772\n",
      "7 100 loss: 0.09295700490474701\n",
      "7 200 loss: 0.09382385015487671\n",
      "7 300 loss: 0.0895531177520752\n",
      "7 400 loss: 0.08645071089267731\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'   #屏蔽无关的信息，2是只打印error\n",
    "#自动查看有没有缓存一个mnist数据集，没有的话自动从google下载。\n",
    "# x: [60k, 28, 28],    x_test: [10k, 28, 28]\n",
    "# y: [60k]             y_test: [10k]\n",
    "(x,y),(x_test,y_test) =datasets.mnist.load_data()\n",
    "\n",
    "# 把数据类型转换为张量\n",
    "#x: [0~255] => [0~ 1.]\n",
    "x=tf.convert_to_tensor(x,dtype=tf.float32) / 255.\n",
    "y=tf.convert_to_tensor(y,dtype=tf.int32)\n",
    "\n",
    "x_test=tf.convert_to_tensor(x_test,dtype=tf.float32) / 255.\n",
    "y_test=tf.convert_to_tensor(y_test,dtype=tf.int32)\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x),tf.reduce_max(x))\n",
    "print(tf.reduce_min(y),tf.reduce_max(y))\n",
    "\n",
    "\n",
    "#创建一个数据集，一次取到一个batch_size,因为上面一次取一个。\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(128)\n",
    "\n",
    "# train_iter = iter(train_db)   #迭代器\n",
    "# sample = next(train_iter)\n",
    "# print(\"batch: \", sample[0].shape, sample[1].shape)\n",
    "\n",
    "#创建权值，完成前向传播\n",
    "#[batch,784] => [b, 256] => [b, 128] => [b, 10]\n",
    "#w: [dim_in, dim_out]\n",
    "#b: [dim_out]\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))#原来均值为0，方差维为1,现在方差变为0.1\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "#前向运算\n",
    "\n",
    "\n",
    "for epoch in range(100):         #iterate 迭代整个数据集10次。\n",
    "    # 外层for: 对所有的图片做循环。\n",
    "    #for (x, y) in train_db:\n",
    "    for step, (x, y) in enumerate(train_db):  #迭代for every batch\n",
    "        #x: [128, 28, 28]\n",
    "        #y: [128]\n",
    "\n",
    "        #维度变换; [b, 28, 28]-> [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        #x: [b, 28*28]\n",
    "        #h1 = x@w1 + b1\n",
    "        #[b, 784]@[784, 256] + [256] = [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "\n",
    "        #使用tensorflow自动求导的过程,其中: w,b。tf.GradientTape()参与梯度计算的代码放到这里面。\n",
    "        with tf.GradientTape() as tape:\n",
    "            #GradientTape里面默认只会跟踪tf.Variable()类型。如果类型不是这个的话。这里为tf.tensor,tf.Variable是tf.tensor的一种特殊类型。\n",
    "            #因此简单的包装一下。\n",
    "\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256]) #直接自动广播机制，也可以手动\n",
    "            h1=tf.nn.relu(h1)\n",
    "            #[b, 256] -> [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2=tf.nn.relu(h2)\n",
    "            #[b, 128] -> [b, 10]\n",
    "            out = h2@w3 + b3  #得到前向输出结果。\n",
    "\n",
    "            #computer loss: 计算误差均方差。\n",
    "            # out维度: [b, 10]\n",
    "            # 真实的y:  [b],维度这里需要把y变成一个one-hot 编码\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "\n",
    "            #mse = mean((y_onehot-out)^2)\n",
    "            #shape : [b, 10]\n",
    "            loss = tf.square(y_onehot - out)\n",
    "\n",
    "            #mean: scalar\n",
    "            #loss = tf.reduce_mean(loss) / b / 10 一般来说除以一个b就够了，每个batch上的一个均值。取决于怎么理解，都是可以的。\n",
    "            loss = tf.reduce_mean(loss) #这里相当于一个放缩，正向放缩不会影响梯度的方向。没有影响的。\n",
    "            #loss = tf.sum(tf.reduce_mean(loss,axis=1)) / b,我自己的理解。\n",
    "\n",
    "        #得到一个梯度。需要求解梯度的有哪些呢？\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        #print(grads)  #结果：[None, None, None, None, None, None]\n",
    "        # w1 = w1 - lr* w1_grad, 这里为了细节，手写。实际可以不用手写。\n",
    "\n",
    "        #w1 = w1 - lr * grads[0]         #这里两个w1为两个对象。原来的w1减去这个值赋值给一个新的对象.w1原来是\n",
    "        \t\t\t\t\t\t\t    #tf.Variable,更新一次之后新的w1变为tf.Tensor类型了。新一次操作之后就会错误。\n",
    "        #b1 = b1 - lr * grads[1]\n",
    "        #w2 = w2 - lr * grads[2]\n",
    "        #b2 = b2 - lr * grads[3]\n",
    "        #w3 = w3 - lr * grads[4]\n",
    "        #b3 = b3 - lr * grads[5]   #第一for增加一个step\n",
    "        w1.assign_sub(lr * grads[0])    #原地更新,数据类型保持不变,\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "\n",
    "        # print(isinstance(b3,tf.Variable))\n",
    "        # print(isinstance(b3,tf.Tensor))\n",
    "        #每100论，看一下loss信息。\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss))\n",
    "\n",
    "\n",
    "    # test/evluation做测试\n",
    "    #注意这里必须使用当前的(最新的) [w1, b1, w2, b2, w3, b3]\n",
    "    total_correct, total_number = 0, 0\n",
    "    for step, (x,y) in enumerate(test_db):\n",
    "\n",
    "        # x_test [b, 28,28] => [b, 28*28]\n",
    "        x_test = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "        # [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "        h1 = tf.nn.relu(x_test@w1 + b1)\n",
    "        h2 = tf.nn.relu(h1@w2 + b2)\n",
    "        out = h2@w3 + b3\n",
    "\n",
    "        # out: [b, 10] 这里的out属于实数的范围R。\n",
    "        # prob: [b, 10]实数范围映射到[0, 1]范围内。\n",
    "        prob = tf.nn.softmax(out, axis=1)  #是在[b, 10]中的10维度上面。故axis=1\n",
    "        # 预测值：选择概率最大的值所在的位置。[b, 10] ==> [b]\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # 真实值：y: [b] 这里我们可以发现做测试的时候，编码方式不需要转换为one-hot,只需要把索引和pred比较。\n",
    "        # [b] int32类型。\n",
    "        # print(pred.dtype, y.dtype) 运行结果：<dtype: 'int64'> <dtype: 'int32'> 类型不匹配\n",
    "        correct = tf.cast(tf.equal(pred, y),dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "\n",
    "        total_correct +=int(correct)        #总的正确的个数，\n",
    "        total_number +=x.shape[0]           #总的测试的个数。\n",
    "\n",
    "\n",
    "    #循环结束以后：\n",
    "    acc = total_correct /total_number\n",
    "    print(\"test acc: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
